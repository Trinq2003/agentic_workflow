{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "from configuration.llm_inference_configuration import APILLMConfiguration\n",
    "from configuration.embedding_inference_configuration import APIEmbeddingModelConfiguration, LocalEmbeddingModelConfiguration\n",
    "from llm.azure_openai import AzureOpenAILLM\n",
    "from embedding.local_embedding import LocalEmbeddingModel\n",
    "from embedding.request_embedding import RequestEmbeddingModel\n",
    "from prompt.zero_shot import ZeroShotPrompt\n",
    "from prompt.few_shot import FewShotPrompt\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format='%(asctime)s\\t\\t%(levelname)s\\t%(message)s',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. System component test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. LLM test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Azure OpenAI test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:04:08,975\t\tDEBUG\t[üîß CONFIGURATION] List of APILLMConfiguration's properties: dict_keys(['llm_id', 'model.model_name', 'model.temperature', 'model.max_tokens', 'cache.enabled', 'cache.cache_expiry', 'llm_api.api_key', 'llm_api.api_base', 'llm_api.api_version', 'llm_api.deployment_name', 'cost.prompt_token_cost', 'cost.response_token_cost', 'retry.max_retries', 'retry.backoff_factor', 'security.encrypted', 'security.trust_strategy'])\n",
      "2025-05-03 15:04:08,976\t\tDEBUG\t[üîß CONFIGURATION] List of APILLMConfiguration's sensitive properties: []\n",
      "2025-05-03 15:04:08,980\t\tDEBUG\tüîß System Component ID: SYSTEM_COMPONENT | 0 | Component type: <class 'llm.azure_openai.AzureOpenAILLM'>\n",
      "2025-05-03 15:04:08,980\t\tINFO\t‚úÖ LLM config loaded: azure_gpt_4o\n",
      "2025-05-03 15:04:08,985\t\tDEBUG\tStarting new HTTPS connection (1): vtnet-ai-service-swedencentral.cognitiveservices.azure.com:443\n",
      "2025-05-03 15:04:10,681\t\tDEBUG\thttps://vtnet-ai-service-swedencentral.cognitiveservices.azure.com:443 \"POST //openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview HTTP/1.1\" 200 956\n",
      "2025-05-03 15:04:10,681\t\tDEBUG\tload_ssl_context verify=True cert=None trust_env=True http2=False\n",
      "2025-05-03 15:04:10,684\t\tDEBUG\tload_verify_locations cafile='C:\\\\Users\\\\ADMIN\\\\miniconda3\\\\Library\\\\ssl\\\\cacert.pem'\n",
      "2025-05-03 15:04:10,883\t\tINFO\t‚úÖ Azure OpenAI model loaded successfully: gpt-4o\n",
      "2025-05-03 15:04:10,884\t\tDEBUG\t[üîß LLM] LLM ID: LLM | azure_gpt_4o\n",
      "2025-05-03 15:04:10,884\t\tDEBUG\tüîß System Component ID: SYSTEM_COMPONENT | 0 | Component type: <class 'llm.azure_openai.AzureOpenAILLM'>\n",
      "2025-05-03 15:04:10,884\t\tINFO\tLLM unique ID test successful\n",
      "2025-05-03 15:04:10,886\t\tERROR\tFailed to create AzureOpenAILLM instance: ‚ùå System Component ID SYSTEM_COMPONENT | 0 is already initiated.\n"
     ]
    }
   ],
   "source": [
    "gpt_4o_azure_configuration = APILLMConfiguration()\n",
    "gpt_4o_azure_configuration.load(\"configuration/yaml/llm/azure_gpt_4o.yaml\")\n",
    "azure_gpt_4o = AzureOpenAILLM(gpt_4o_azure_configuration)\n",
    "\n",
    "try:\n",
    "    azure_gpt_4o_1 = AzureOpenAILLM(gpt_4o_azure_configuration)\n",
    "except Exception as e:\n",
    "    logging.info(\"LLM unique ID test successful\")\n",
    "    logging.error(f\"Failed to create AzureOpenAILLM instance: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:06:27,592\t\tDEBUG\tRequest options: {'method': 'post', 'url': '/deployments/gpt-4o/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'user', 'content': \"Health check. Say 'hi' to start the conversation.\"}], 'model': 'gpt-4o', 'max_tokens': 2048, 'n': 2, 'stop': ['\\n'], 'temperature': 0.2}}\n",
      "2025-04-29 16:06:27,594\t\tDEBUG\tSending HTTP Request: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview\n",
      "2025-04-29 16:06:27,595\t\tDEBUG\tconnect_tcp.started host='vtnet-ai-service-swedencentral.cognitiveservices.azure.com' port=443 local_address=None timeout=5.0 socket_options=None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:06:27,862\t\tDEBUG\tconnect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021938A6EDD0>\n",
      "2025-04-29 16:06:27,863\t\tDEBUG\tstart_tls.started ssl_context=<ssl.SSLContext object at 0x0000021936125370> server_hostname='vtnet-ai-service-swedencentral.cognitiveservices.azure.com' timeout=5.0\n",
      "2025-04-29 16:06:28,302\t\tDEBUG\tstart_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x0000021938A7BC90>\n",
      "2025-04-29 16:06:28,303\t\tDEBUG\tsend_request_headers.started request=<Request [b'POST']>\n",
      "2025-04-29 16:06:28,304\t\tDEBUG\tsend_request_headers.complete\n",
      "2025-04-29 16:06:28,304\t\tDEBUG\tsend_request_body.started request=<Request [b'POST']>\n",
      "2025-04-29 16:06:28,306\t\tDEBUG\tsend_request_body.complete\n",
      "2025-04-29 16:06:28,306\t\tDEBUG\treceive_response_headers.started request=<Request [b'POST']>\n",
      "2025-04-29 16:06:28,817\t\tDEBUG\treceive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'1745'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'046597ec-0d69-438f-834e-3fc60f646c13'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ms-region', b'Sweden Central'), (b'x-ratelimit-remaining-requests', b'149'), (b'x-ratelimit-limit-requests', b'150'), (b'x-ratelimit-remaining-tokens', b'145308'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'76010138-faa1-48ba-94e7-bb740b8c433d'), (b'x-ms-client-request-id', b'046597ec-0d69-438f-834e-3fc60f646c13'), (b'azureml-model-session', b'v20250415-5-168444713-3'), (b'x-ms-deployment-name', b'gpt-4o'), (b'Date', b'Tue, 29 Apr 2025 09:06:28 GMT')])\n",
      "2025-04-29 16:06:28,819\t\tINFO\tHTTP Request: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-29 16:06:28,819\t\tDEBUG\treceive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-29 16:06:28,820\t\tDEBUG\treceive_response_body.complete\n",
      "2025-04-29 16:06:28,821\t\tDEBUG\tresponse_closed.started\n",
      "2025-04-29 16:06:28,821\t\tDEBUG\tresponse_closed.complete\n",
      "2025-04-29 16:06:28,821\t\tDEBUG\tHTTP Response: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'content-length': '1745', 'content-type': 'application/json', 'apim-request-id': '046597ec-0d69-438f-834e-3fc60f646c13', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'Sweden Central', 'x-ratelimit-remaining-requests': '149', 'x-ratelimit-limit-requests': '150', 'x-ratelimit-remaining-tokens': '145308', 'x-ratelimit-limit-tokens': '150000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '76010138-faa1-48ba-94e7-bb740b8c433d', 'x-ms-client-request-id': '046597ec-0d69-438f-834e-3fc60f646c13', 'azureml-model-session': 'v20250415-5-168444713-3', 'x-ms-deployment-name': 'gpt-4o', 'date': 'Tue, 29 Apr 2025 09:06:28 GMT'})\n",
      "2025-04-29 16:06:28,823\t\tDEBUG\trequest_id: 76010138-faa1-48ba-94e7-bb740b8c433d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi! How can I assist you with your health check today?', 'Hi! How can I assist you with your health check today?']\n"
     ]
    }
   ],
   "source": [
    "sample_zero_shot_message = ZeroShotPrompt(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Health check. Say 'hi' to start the conversation.\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_zero_shot_message_responses = azure_gpt_4o.query(query=sample_zero_shot_message.prompt, num_responses=2)\n",
    "print(azure_gpt_4o.get_response_texts(sample_zero_shot_message_responses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-29 16:07:02,379\t\tDEBUG\tRequest options: {'method': 'post', 'url': '/deployments/gpt-4o/chat/completions', 'headers': {'api-key': '<redacted>'}, 'files': None, 'json_data': {'messages': [{'role': 'system', 'content': 'You are NetMind assistant. You task is to answer to the user anything about Viettel Group.'}, {'role': 'user', 'content': 'Tell me about Viettel Group.'}, {'role': 'assistant', 'content': 'Viettel Group is a Vietnamese multinational telecommunications company headquartered in Hanoi, Vietnam. It is a state-owned enterprise and operated by the Ministry of Defence. You can find out more about Viettel Group at https://viettel.vn/.'}, {'role': 'user', 'content': 'What is the revenue of Viettel Group?'}], 'model': 'gpt-4o', 'max_tokens': 2048, 'n': 2, 'stop': ['\\n'], 'temperature': 0.2}}\n",
      "2025-04-29 16:07:02,380\t\tDEBUG\tSending HTTP Request: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview\n",
      "2025-04-29 16:07:02,381\t\tDEBUG\tclose.started\n",
      "2025-04-29 16:07:02,382\t\tDEBUG\tclose.complete\n",
      "2025-04-29 16:07:02,383\t\tDEBUG\tconnect_tcp.started host='vtnet-ai-service-swedencentral.cognitiveservices.azure.com' port=443 local_address=None timeout=5.0 socket_options=None\n",
      "2025-04-29 16:07:02,653\t\tDEBUG\tconnect_tcp.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002193662B010>\n",
      "2025-04-29 16:07:02,654\t\tDEBUG\tstart_tls.started ssl_context=<ssl.SSLContext object at 0x0000021936125370> server_hostname='vtnet-ai-service-swedencentral.cognitiveservices.azure.com' timeout=5.0\n",
      "2025-04-29 16:07:03,118\t\tDEBUG\tstart_tls.complete return_value=<httpcore._backends.sync.SyncStream object at 0x000002193783CC50>\n",
      "2025-04-29 16:07:03,118\t\tDEBUG\tsend_request_headers.started request=<Request [b'POST']>\n",
      "2025-04-29 16:07:03,120\t\tDEBUG\tsend_request_headers.complete\n",
      "2025-04-29 16:07:03,122\t\tDEBUG\tsend_request_body.started request=<Request [b'POST']>\n",
      "2025-04-29 16:07:03,122\t\tDEBUG\tsend_request_body.complete\n",
      "2025-04-29 16:07:03,122\t\tDEBUG\treceive_response_headers.started request=<Request [b'POST']>\n",
      "2025-04-29 16:07:04,593\t\tDEBUG\treceive_response_headers.complete return_value=(b'HTTP/1.1', 200, b'OK', [(b'Content-Length', b'2259'), (b'Content-Type', b'application/json'), (b'apim-request-id', b'88f0765c-7609-40e7-9758-4f5cb5f9f5a9'), (b'Strict-Transport-Security', b'max-age=31536000; includeSubDomains; preload'), (b'x-content-type-options', b'nosniff'), (b'x-ms-region', b'Sweden Central'), (b'x-ratelimit-remaining-requests', b'149'), (b'x-ratelimit-limit-requests', b'150'), (b'x-ratelimit-remaining-tokens', b'141578'), (b'x-ratelimit-limit-tokens', b'150000'), (b'x-accel-buffering', b'no'), (b'x-ms-rai-invoked', b'true'), (b'x-request-id', b'9a9f38a6-7022-497a-8a48-d957437b8b51'), (b'x-ms-client-request-id', b'88f0765c-7609-40e7-9758-4f5cb5f9f5a9'), (b'azureml-model-session', b'v20250415-5-168444713-5'), (b'x-ms-deployment-name', b'gpt-4o'), (b'Date', b'Tue, 29 Apr 2025 09:07:04 GMT')])\n",
      "2025-04-29 16:07:04,595\t\tINFO\tHTTP Request: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-04-29 16:07:04,595\t\tDEBUG\treceive_response_body.started request=<Request [b'POST']>\n",
      "2025-04-29 16:07:04,596\t\tDEBUG\treceive_response_body.complete\n",
      "2025-04-29 16:07:04,597\t\tDEBUG\tresponse_closed.started\n",
      "2025-04-29 16:07:04,597\t\tDEBUG\tresponse_closed.complete\n",
      "2025-04-29 16:07:04,597\t\tDEBUG\tHTTP Response: POST https://vtnet-ai-service-swedencentral.cognitiveservices.azure.com/openai/deployments/gpt-4o/chat/completions?api-version=2025-01-01-preview \"200 OK\" Headers({'content-length': '2259', 'content-type': 'application/json', 'apim-request-id': '88f0765c-7609-40e7-9758-4f5cb5f9f5a9', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'Sweden Central', 'x-ratelimit-remaining-requests': '149', 'x-ratelimit-limit-requests': '150', 'x-ratelimit-remaining-tokens': '141578', 'x-ratelimit-limit-tokens': '150000', 'x-accel-buffering': 'no', 'x-ms-rai-invoked': 'true', 'x-request-id': '9a9f38a6-7022-497a-8a48-d957437b8b51', 'x-ms-client-request-id': '88f0765c-7609-40e7-9758-4f5cb5f9f5a9', 'azureml-model-session': 'v20250415-5-168444713-5', 'x-ms-deployment-name': 'gpt-4o', 'date': 'Tue, 29 Apr 2025 09:07:04 GMT'})\n",
      "2025-04-29 16:07:04,599\t\tDEBUG\trequest_id: 9a9f38a6-7022-497a-8a48-d957437b8b51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"As of the latest available data, Viettel Group reported a revenue of approximately 274 trillion Vietnamese Dong (VND) in 2022. However, please note that financial figures can vary year by year, and it's advisable to check the most recent reports or official announcements for the latest figures.\", \"As of the latest available data, Viettel Group reported a revenue of approximately 274 trillion VND (Vietnamese Dong) in 2022, which is roughly equivalent to around 11.8 billion USD. This figure reflects Viettel's strong position in the telecommunications industry, both domestically and internationally. Keep in mind that financial figures can vary year by year, so it's always a good idea to check the most recent reports for updated information.\"]\n"
     ]
    }
   ],
   "source": [
    "sample_few_shots_message = FewShotPrompt(\n",
    "    [\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are NetMind assistant. You task is to answer to the user anything about Viettel Group.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"Tell me about Viettel Group.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"Viettel Group is a Vietnamese multinational telecommunications company headquartered in Hanoi, Vietnam. It is a state-owned enterprise and operated by the Ministry of Defence. You can find out more about Viettel Group at https://viettel.vn/.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"What is the revenue of Viettel Group?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "sample_few_shots_message_responses = azure_gpt_4o.query(query=sample_few_shots_message.prompt, num_responses=2)\n",
    "print(azure_gpt_4o.get_response_texts(sample_few_shots_message_responses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Embedding test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:40:23,628\t\tDEBUG\t[üîß APIEmbeddingModelConfiguration] List of APIEmbeddingModelConfiguration's properties: dict_keys(['emb_id', 'model.model_name', 'model.max_tokens', 'model.embedding_dims', 'model.identical_threshold', 'emb_api.api_base', 'emb_api.api_token', 'emb_api.trust_remote_code', 'cost.prompt_token_cost', 'cost.response_token_cost', 'retry.max_retries', 'retry.backoff_factor'])\n",
      "2025-05-03 15:40:23,628\t\tDEBUG\t[üîß APIEmbeddingModelConfiguration] List of APIEmbeddingModelConfiguration's sensitive properties: []\n",
      "2025-05-03 15:40:23,643\t\tDEBUG\t[üîß RequestEmbeddingModel] System Component ID: SYSTEM_COMPONENT | 0 | Component type: <class 'embedding.request_embedding.RequestEmbeddingModel'>\n",
      "2025-05-03 15:40:23,644\t\tINFO\t[‚úÖ RequestEmbeddingModel] Embedding config loaded: all-MiniLM-L6-v2\n",
      "2025-05-03 15:40:23,644\t\tDEBUG\tList of APIEmbeddingModelConfiguration: {'_properties': {'emb_id': {'default-value': '', 'transform_fn': <class 'str'>}, 'model.model_name': {'default-value': '', 'transform_fn': <class 'str'>}, 'model.max_tokens': {'default-value': 0, 'transform_fn': <class 'int'>}, 'model.embedding_dims': {'default-value': 0, 'transform_fn': <class 'int'>}, 'model.identical_threshold': {'default-value': 0.999, 'transform_fn': <class 'float'>}, 'emb_api.api_base': {'default-value': '', 'transform_fn': <class 'str'>}, 'emb_api.api_token': {'default-value': '', 'transform_fn': <class 'str'>}, 'emb_api.trust_remote_code': {'default-value': False, 'transform_fn': <class 'bool'>}, 'cost.prompt_token_cost': {'default-value': 0.0, 'transform_fn': <class 'float'>}, 'cost.response_token_cost': {'default-value': 0.0, 'transform_fn': <class 'float'>}, 'retry.max_retries': {'default-value': 3, 'transform_fn': <class 'int'>}, 'retry.backoff_factor': {'default-value': 0.1, 'transform_fn': <class 'float'>}}, '_sensitive_properties': ['emb_api_api_token'], 'emb_id': 'all-MiniLM-L6-v2', 'model_model_name': 'all-MiniLM-L6-v2', 'model_max_tokens': 1024, 'model_embedding_dims': 8192, 'model_identical_threshold': 0.9, 'emb_api_api_token': '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', 'emb_api_api_base': 'https://api-inference.huggingface.co/models/all-MiniLM-L6-v2', 'emb_api_trust_remote_code': True, 'cost_prompt_token_cost': 0.03, 'cost_response_token_cost': 0.06, 'retry_max_retries': 5, 'retry_backoff_factor': 2.0}\n",
      "2025-05-03 15:40:23,649\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:40:24,521\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"GET /models/all-MiniLM-L6-v2 HTTP/1.1\" 404 49\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[‚ùå RequestEmbeddingModel] Failed to connect to API embedding model: 404, {\"error\":\"Model all-MiniLM-L6-v2 does not exist\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m all_mini_v2_configuration \u001b[38;5;241m=\u001b[39m APIEmbeddingModelConfiguration()\n\u001b[0;32m      2\u001b[0m all_mini_v2_configuration\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfiguration/yaml/embedding/all-MiniLM-L6-v2.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m bge_m3_hf \u001b[38;5;241m=\u001b[39m RequestEmbeddingModel(all_mini_v2_configuration)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Code\\VTNET\\agentic_workflow\\embedding\\request_embedding.py:31\u001b[0m, in \u001b[0;36mRequestEmbeddingModel.__init__\u001b[1;34m(self, embedding_model_config)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__retry_max_retries: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mretry_max_retries\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__retry_backoff_factor: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mretry_backoff_factor\n\u001b[1;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_model()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Code\\VTNET\\agentic_workflow\\embedding\\request_embedding.py:43\u001b[0m, in \u001b[0;36mRequestEmbeddingModel._load_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     41\u001b[0m response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__emb_api_api_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, headers\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Failed to connect to API embedding model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[‚úÖ \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] API embedding model loaded successfully from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__emb_api_api_base\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: [‚ùå RequestEmbeddingModel] Failed to connect to API embedding model: 404, {\"error\":\"Model all-MiniLM-L6-v2 does not exist\"}"
     ]
    }
   ],
   "source": [
    "all_mini_v2_configuration = APIEmbeddingModelConfiguration()\n",
    "all_mini_v2_configuration.load(\"configuration/yaml/embedding/all-MiniLM-L6-v2.yaml\")\n",
    "bge_m3_hf = RequestEmbeddingModel(all_mini_v2_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:16:01,399\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:16:03,409\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"POST /models/BAAI/bge-m3 HTTP/1.1\" 503 None\n",
      "2025-05-03 15:16:03,412\t\tWARNING\t[‚ùå RequestEmbeddingModel] HTTP error occurred: 503 Server Error: Service Temporarily Unavailable for url: https://api-inference.huggingface.co/models/BAAI/bge-m3. Retrying 1/5...\n",
      "2025-05-03 15:16:05,416\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:16:06,253\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"POST /models/BAAI/bge-m3 HTTP/1.1\" 503 None\n",
      "2025-05-03 15:16:06,255\t\tWARNING\t[‚ùå RequestEmbeddingModel] HTTP error occurred: 503 Server Error: Service Temporarily Unavailable for url: https://api-inference.huggingface.co/models/BAAI/bge-m3. Retrying 2/5...\n",
      "2025-05-03 15:16:10,258\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:16:12,228\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"POST /models/BAAI/bge-m3 HTTP/1.1\" 503 None\n",
      "2025-05-03 15:16:12,232\t\tWARNING\t[‚ùå RequestEmbeddingModel] HTTP error occurred: 503 Server Error: Service Temporarily Unavailable for url: https://api-inference.huggingface.co/models/BAAI/bge-m3. Retrying 3/5...\n",
      "2025-05-03 15:16:20,236\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:16:21,056\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"POST /models/BAAI/bge-m3 HTTP/1.1\" 503 None\n",
      "2025-05-03 15:16:21,058\t\tWARNING\t[‚ùå RequestEmbeddingModel] HTTP error occurred: 503 Server Error: Service Temporarily Unavailable for url: https://api-inference.huggingface.co/models/BAAI/bge-m3. Retrying 4/5...\n",
      "2025-05-03 15:16:37,061\t\tDEBUG\tStarting new HTTPS connection (1): api-inference.huggingface.co:443\n",
      "2025-05-03 15:16:39,229\t\tDEBUG\thttps://api-inference.huggingface.co:443 \"POST /models/BAAI/bge-m3 HTTP/1.1\" 503 None\n",
      "2025-05-03 15:16:39,233\t\tWARNING\t[‚ùå RequestEmbeddingModel] HTTP error occurred: 503 Server Error: Service Temporarily Unavailable for url: https://api-inference.huggingface.co/models/BAAI/bge-m3. Retrying 5/5...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[‚ùå RequestEmbeddingModel] Max retries exceeded. Failed to get embedding.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bge_m3_hf\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello world\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Code\\VTNET\\agentic_workflow\\embedding\\request_embedding.py:80\u001b[0m, in \u001b[0;36mRequestEmbeddingModel.encode\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Retrying \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mattempt\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__retry_max_retries\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     79\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__retry_backoff_factor \u001b[38;5;241m*\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m attempt))  \u001b[38;5;66;03m# Exponential backoff\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[‚ùå \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Max retries exceeded. Failed to get embedding.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: [‚ùå RequestEmbeddingModel] Max retries exceeded. Failed to get embedding."
     ]
    }
   ],
   "source": [
    "bge_m3_hf.encode(\"Hello world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-03 15:06:10,099\t\tDEBUG\tüîß System Component ID: SYSTEM_COMPONENT | 0 | Component type: <class 'embedding.request_embedding.RequestEmbeddingModel'>\n",
      "2025-05-03 15:06:10,101\t\tINFO\t‚úÖ Embedding config loaded: bge-m3\n",
      "2025-05-03 15:06:10,101\t\tDEBUG\tList of APIEmbeddingModelConfiguration: {'_properties': {'emb_id': {'default-value': '', 'transform_fn': <class 'str'>}, 'model.model_name': {'default-value': '', 'transform_fn': <class 'str'>}, 'model.max_tokens': {'default-value': 0, 'transform_fn': <class 'int'>}, 'model.embedding_dims': {'default-value': 0, 'transform_fn': <class 'int'>}, 'model.identical_threshold': {'default-value': 0.999, 'transform_fn': <class 'float'>}, 'emb_api.api_base': {'default-value': '', 'transform_fn': <class 'str'>}, 'emb_api.api_token': {'default-value': '', 'transform_fn': <class 'str'>}, 'emb_api.trust_remote_code': {'default-value': False, 'transform_fn': <class 'bool'>}, 'cost.prompt_token_cost': {'default-value': 0.0, 'transform_fn': <class 'float'>}, 'cost.response_token_cost': {'default-value': 0.0, 'transform_fn': <class 'float'>}, 'retry.max_retries': {'default-value': 3, 'transform_fn': <class 'int'>}, 'retry.backoff_factor': {'default-value': 0.1, 'transform_fn': <class 'float'>}}, '_sensitive_properties': ['emb_api_api_token'], 'emb_id': 'bge-m3', 'model_model_name': 'BAAI/bge-m3', 'model_max_tokens': 1024, 'model_embedding_dims': 8192, 'model_identical_threshold': 0.9, 'emb_api_api_key': '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', 'emb_api_api_base': 'https://api-inference.huggingface.co/models/BAAI/bge-m3', 'emb_api_trust_remote_code': True, 'cost_prompt_token_cost': 0.03, 'cost_response_token_cost': 0.06, 'retry_max_retries': 5, 'retry_backoff_factor': 2.0}\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'APIEmbeddingModelConfiguration' object has no attribute 'emb_api_api_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bge_m3_hf \u001b[38;5;241m=\u001b[39m RequestEmbeddingModel(bge_m3_configuration)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Code\\VTNET\\agentic_workflow\\embedding\\request_embedding.py:24\u001b[0m, in \u001b[0;36mRequestEmbeddingModel.__init__\u001b[1;34m(self, embedding_model_config)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mList of APIEmbeddingModelConfiguration: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00membedding_model_config\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__emb_api_api_base: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39memb_api_api_base\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__emb_api_api_token: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39memb_api_api_token\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__emb_api_trust_remote_code: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39memb_api_trust_remote_code\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__cost_prompt_token_cost: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mcost_prompt_token_cost\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'APIEmbeddingModelConfiguration' object has no attribute 'emb_api_api_token'"
     ]
    }
   ],
   "source": [
    "bge_m3_hf = RequestEmbeddingModel(bge_m3_configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['b', 'c']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {\"b\": 1, \"c\": 2}\n",
    "list(a.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Python code runner tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.python_code_runner import PythonCodeRunnerTool\n",
    "from configuration.tool_configuration import ToolConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_code_runner_config = ToolConfiguration()\n",
    "python_code_runner_config.load(\"configuration/yaml/tools/python_code_runner.yaml\")\n",
    "python_code_runner = PythonCodeRunnerTool(python_code_runner_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_code_list = [\n",
    "    {\n",
    "        \"code_string\": \"a = 11\\nb = 20\\nresult = a + b\\n\\ndef greet(name):\\n return f'Hello, {name}!'\\n \\nmessage = greet('World')\",\n",
    "        \"id\": \"1\"\n",
    "    },\n",
    "    {\n",
    "        \"code_string\": \"def multiply(x, y):\\n\\treturn x * y\\n\\nnum1 = 5\\nnum2 = 3\\nresult = multiply(num1, num2)\\n\\nprint(f'The product of {num1} and {num2} is {result}.')\",\n",
    "        \"id\": \"2\"\n",
    "    }\n",
    "]\n",
    "\n",
    "results = python_code_runner.execute(input_code_list=input_code_list)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configuration.llm_inference_configuration import APILLMConfiguration\n",
    "from llm.azure_openai import AzureOpenAILLM\n",
    "from base_classes.memory.memory_atom import AbstractMemoryAtom\n",
    "from base_classes.memory.memory_block import AbstractMemoryBlock\n",
    "from base_classes.memory.memory import AbstractMemory\n",
    "from base_classes.memory.datatypes.data_item import PromptDataItem\n",
    "from prompt.user_message import UserMessagePrompt\n",
    "from prompt.assistant_message import AssistantMessagePrompt\n",
    "from base_classes.memory.memory_feature_engineer import MemoryFeatureEngineer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_data_1 = [{\n",
    "    'role': 'user',\n",
    "    'content': 'Calculate the sum of 11 and 20.'\n",
    "}]\n",
    "mem_atom_1 = AbstractMemoryAtom(data=PromptDataItem(UserMessagePrompt(prompt_data_1)))\n",
    "\n",
    "prompt_data_2 = [{\n",
    "    'role': 'assistant',\n",
    "    'content': 'In order to calculate the sum of two numbers, we need to do the following:\\n\\t1. Write a Python code with add_sum() function, receiving 2 variables.\\n\\t2. Execute the code with the given variables.\\n\\t3. Return the result of the sum.'\n",
    "}]\n",
    "mem_atom_2 = AbstractMemoryAtom(data=PromptDataItem(AssistantMessagePrompt(prompt_data_2)))\n",
    "\n",
    "prompt_data_3 = [{\n",
    "    'role': 'assistant',\n",
    "    'content': 'Here is the Python code to calculate the sum of two numbers:\\n\\na = 11\\nb = 20\\nresult = a + b\\n\\nprint(f\"The sum of {a} and {b} is {result}.\")'\n",
    "}]\n",
    "mem_atom_3 = AbstractMemoryAtom(data=PromptDataItem(AssistantMessagePrompt(prompt_data_3)))\n",
    "\n",
    "prompt_data_4 = [{\n",
    "    'role': 'assistant',\n",
    "    'content': 'After executing the code, we got the sum of 11 and 20 is 31.'\n",
    "}]\n",
    "mem_atom_4 = AbstractMemoryAtom(data=PromptDataItem(AssistantMessagePrompt(prompt_data_4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[UUID('502d1013-044e-46c0-b3e1-755f5e4cfd1c'),\n",
       " UUID('a13de8b8-ec81-4af4-a6bf-33f6e7c93b33'),\n",
       " UUID('ed6b3e6e-bcd4-4a97-8566-7fd562983cd3'),\n",
       " UUID('18fd9901-2553-43fe-bbf6-f9ce14418a89')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AbstractMemoryAtom.get_mematom_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mem_block = AbstractMemoryBlock()\n",
    "mem_block.add_memory_atom(mem_atom_1)\n",
    "mem_block.add_memory_atom(mem_atom_2)\n",
    "mem_block.add_memory_atom(mem_atom_3)\n",
    "mem_block.add_memory_atom(mem_atom_4)\n",
    "\n",
    "mem_block.mem_atom_graph = {\n",
    "        mem_atom_1.mem_atom_id: [mem_atom_2.mem_atom_id],\n",
    "        mem_atom_2.mem_atom_id: [mem_atom_3.mem_atom_id, mem_atom_4.mem_atom_id],\n",
    "        mem_atom_3.mem_atom_id: [],\n",
    "        mem_atom_4.mem_atom_id: []\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AbstractMemory' object has no attribute '_memory_fe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m memory \u001b[38;5;241m=\u001b[39m AbstractMemory()\n\u001b[1;32m----> 2\u001b[0m memory\u001b[38;5;241m.\u001b[39madd_memory_block(mem_block)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\Code\\VTNET\\agentic_workflow\\base_classes\\memory\\memory.py:51\u001b[0m, in \u001b[0;36mAbstractMemory.add_memory_block\u001b[1;34m(self, memory_block)\u001b[0m\n\u001b[0;32m     49\u001b[0m memory_block\u001b[38;5;241m.\u001b[39mblock_address_in_memory \u001b[38;5;241m=\u001b[39m block_address\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_blocks[memory_block\u001b[38;5;241m.\u001b[39mmem_block_id] \u001b[38;5;241m=\u001b[39m AbstractMemoryBlock\u001b[38;5;241m.\u001b[39mget_memblock_instance_by_id(memory_block\u001b[38;5;241m.\u001b[39mmem_block_id)\n\u001b[1;32m---> 51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_memory_fe\u001b[38;5;241m.\u001b[39mmemory_feature_engineering(memory_block_id \u001b[38;5;241m=\u001b[39m memory_block\u001b[38;5;241m.\u001b[39mmem_block_id)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AbstractMemory' object has no attribute '_memory_fe'"
     ]
    }
   ],
   "source": [
    "memory = AbstractMemory()\n",
    "memory.add_memory_block(mem_block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory.get_memory_block_by_id(mem_block.mem_block_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_a',\n",
       " 'description': 'This is the get_a method of class A.',\n",
       " 'parameters': {'properties': {'m': {'type': 'integer'}},\n",
       "  'required': ['m'],\n",
       "  'type': 'object'}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from annotated_docs.json_schema import as_json_schema\n",
    "\n",
    "class A:\n",
    "    \"\"\"\n",
    "    This is a class A.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, a: int, b: str):\n",
    "        \"\"\"\n",
    "        This is the constructor of class A.\n",
    "        \"\"\"\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "    @classmethod\n",
    "    def get_a(self, m: int) -> int:\n",
    "        \"\"\"\n",
    "        This is the get_a method of class A.\n",
    "        \"\"\"\n",
    "        return self.a\n",
    "    \n",
    "    def get_b(self) -> str:\n",
    "        \"\"\"\n",
    "        This is the get_b method of class A.\n",
    "        \"\"\"\n",
    "        return self.b\n",
    "\n",
    "a = A(a = 2, b = \"a\")\n",
    "\n",
    "as_json_schema(A.get_a)\n",
    "# a = A(a = 2, b = \"a\")\n",
    "# as_json_schema(A.get_a())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
